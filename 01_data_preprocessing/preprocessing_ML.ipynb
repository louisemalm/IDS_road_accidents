{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing for machine learning\n",
    "\n",
    "For the machine learning task, we will only use the collisions dataset, and only features that might be known before an accident happens. Morover, we will only use the data from years 2000 - 2022 for this task. Thus, the following steps will be taken in this notebook:\n",
    "1. Read in the data\n",
    "2. Filter the data based on years\n",
    "3. Remove columns\n",
    "4. Rename categorical values\n",
    "5. Binning (speed limits and time)\n",
    "6. Remove unknowns\n",
    "7. Split the data into train, validation, and test sets\n",
    "8. One-hot encoding\n",
    "9. Balance the train data\n",
    "10. Save the train, validation, and test sets as .csv files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Read in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from joblib import dump\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_data_ML = pd.read_csv('../00_data/UK_road_casualty_collision_1979_2022.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8809915, 36)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_data_ML.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Filter the data based on years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all records prior to year 2000\n",
    "acc_data_ML = acc_data_ML[acc_data_ML['accident_year'] >= 2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3691651, 36)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_data_ML.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Remove columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 3691651 entries, 5118264 to 8809914\n",
      "Data columns (total 36 columns):\n",
      " #   Column                                       Dtype  \n",
      "---  ------                                       -----  \n",
      " 0   accident_index                               object \n",
      " 1   accident_year                                int64  \n",
      " 2   accident_reference                           object \n",
      " 3   location_easting_osgr                        float64\n",
      " 4   location_northing_osgr                       float64\n",
      " 5   longitude                                    float64\n",
      " 6   latitude                                     float64\n",
      " 7   police_force                                 int64  \n",
      " 8   accident_severity                            int64  \n",
      " 9   number_of_vehicles                           int64  \n",
      " 10  number_of_casualties                         int64  \n",
      " 11  date                                         object \n",
      " 12  day_of_week                                  int64  \n",
      " 13  time                                         object \n",
      " 14  local_authority_district                     int64  \n",
      " 15  local_authority_ons_district                 object \n",
      " 16  local_authority_highway                      object \n",
      " 17  first_road_class                             int64  \n",
      " 18  first_road_number                            int64  \n",
      " 19  road_type                                    int64  \n",
      " 20  speed_limit                                  float64\n",
      " 21  junction_detail                              int64  \n",
      " 22  junction_control                             int64  \n",
      " 23  second_road_class                            int64  \n",
      " 24  second_road_number                           int64  \n",
      " 25  pedestrian_crossing_human_control            int64  \n",
      " 26  pedestrian_crossing_physical_facilities      int64  \n",
      " 27  light_conditions                             int64  \n",
      " 28  weather_conditions                           int64  \n",
      " 29  road_surface_conditions                      int64  \n",
      " 30  special_conditions_at_site                   int64  \n",
      " 31  carriageway_hazards                          int64  \n",
      " 32  urban_or_rural_area                          int64  \n",
      " 33  did_police_officer_attend_scene_of_accident  int64  \n",
      " 34  trunk_road_flag                              int64  \n",
      " 35  lsoa_of_accident_location                    object \n",
      "dtypes: float64(5), int64(24), object(7)\n",
      "memory usage: 1.0+ GB\n"
     ]
    }
   ],
   "source": [
    "acc_data_ML.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the columns with (for ML) non-relevant information and columns with information that's unknown before an \n",
    "# accident occurs\n",
    "cols_remove = ['accident_index', 'accident_year', 'accident_reference', 'location_easting_osgr', 'location_northing_osgr', \n",
    "               'longitude', 'latitude', 'police_force', 'number_of_vehicles', 'number_of_casualties', 'date',\n",
    "               'local_authority_district', 'local_authority_ons_district', 'local_authority_highway', 'first_road_number', \n",
    "               'second_road_number', 'pedestrian_crossing_human_control', 'pedestrian_crossing_physical_facilities',\n",
    "               'did_police_officer_attend_scene_of_accident', 'trunk_road_flag', 'lsoa_of_accident_location']\n",
    "\n",
    "\n",
    "acc_data_ML = acc_data_ML.drop(cols_remove, axis = 1)\n",
    "\n",
    "# Replace nan values with -1\n",
    "acc_data_ML.fillna(-1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3691651, 15)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_data_ML.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 3691651 entries, 5118264 to 8809914\n",
      "Data columns (total 15 columns):\n",
      " #   Column                      Dtype  \n",
      "---  ------                      -----  \n",
      " 0   accident_severity           int64  \n",
      " 1   day_of_week                 int64  \n",
      " 2   time                        object \n",
      " 3   first_road_class            int64  \n",
      " 4   road_type                   int64  \n",
      " 5   speed_limit                 float64\n",
      " 6   junction_detail             int64  \n",
      " 7   junction_control            int64  \n",
      " 8   second_road_class           int64  \n",
      " 9   light_conditions            int64  \n",
      " 10  weather_conditions          int64  \n",
      " 11  road_surface_conditions     int64  \n",
      " 12  special_conditions_at_site  int64  \n",
      " 13  carriageway_hazards         int64  \n",
      " 14  urban_or_rural_area         int64  \n",
      "dtypes: float64(1), int64(13), object(1)\n",
      "memory usage: 450.6+ MB\n"
     ]
    }
   ],
   "source": [
    "# Check all unimportant features were removed\n",
    "acc_data_ML.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Rename categorical values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary for renaming the features\n",
    "\n",
    "# Only interested in predicting fatal or non-fatal accidents, thus grouping severity 2 and 3 (severe and slight) into one\n",
    "# group (0) and fatal as 1\n",
    "accident_severity_dict = { \n",
    "    1: 1, 2: 0, 3: 0\n",
    "    }\n",
    "day_of_week_dict = {\n",
    "    1: 'sunday', 2: 'monday', 3: 'tuesday', 4: 'wednesday', 5: 'thursday', 6: 'friday', 7: 'saturday'\n",
    "    }\n",
    "first_road_class_dict = {\n",
    "    1: 'motorway', 2: 'motorway', 3: 'A', 4: 'B', 5: 'C', 6: 'unknown', -1: 'unknown'\n",
    "    }\n",
    "road_type_dict = {\n",
    "    1: 'roundabout', 2: 'one_way_or_slip_road', 3: 'dual_carriageway', 6: 'single_carriageway', 7: 'one_way_or_slip_road', \n",
    "    9: 'unknown', 12: 'one_way_or_slip_road', -1: 'unknown'\n",
    "    }\n",
    "junction_detail_dict = {\n",
    "    0: 'not_a_junction', 1: 'roundabout', 2: 'roundabout', 3: 'tor_staggered_junction', 5: 'slip_road', 6: 'crossroads', \n",
    "    7: 'more_than_4_arms', 8: 'private_drive_or_entrance', 9: 'other_junction', 99: 'unknown', -1: 'unknown'\n",
    "    }\n",
    "junction_control_dict = {\n",
    "    0: 'not_a_junction', 1: 'authorised_person', 2: 'auto_traffic_signal', 3: 'stop_sign', 4: 'give_way_or_uncontrolled', \n",
    "    -1: 'unknown', 9: 'unknown'\n",
    "    }\n",
    "second_road_class_dict = {\n",
    "    0: 'not_a_junction', 1: 'motorway', 2: 'motorway', 3: 'A', 4: 'B', 5: 'C', 6: 'unknown', 9: 'unknown', -1: 'unknown'\n",
    "    }\n",
    "light_conditions_dict = {\n",
    "    1: 'daylight', 4: 'dark', 5: 'dark', 6: 'dark', 7: 'dark', -1: 'unknown'\n",
    "    }\n",
    "weather_conditions_dict = {\n",
    "    1: 'fine_no_high_winds', 2: 'raining_no_high_winds', 3: 'snowing_no_high_winds', 4: 'fine_high_winds', \n",
    "    5: 'raining_high_winds', 6: 'snowing_high_winds', 7: 'fog_or_mist', 8: 'other_weather', 9: 'unknown', -1: 'unknown'\n",
    "    }\n",
    "road_surface_conditions_dict = {\n",
    "    1: 'dry', 2: 'wet_or_damp', 3: 'snow', 4: 'frost_or_ice', 5: 'flood_over_3cm', 6: 'oil_or_diesel_road', 7: 'mud_road', \n",
    "    -1: 'unknown', 9: 'unknown'\n",
    "    }\n",
    "special_conditions_at_site_dict = {\n",
    "    0: 'no_special_conditions_at_site', 1: 'auto_traffic_signal_out', 2: 'auto_signal_part_defective', \n",
    "    3: 'road_sign_or_marking_defective_or_obscured', 4: 'roadworks', 5: 'road_surface_defective', 6: 'oil_or_diesel_site', \n",
    "    7: 'mud_site', -1: 'unknown', 9: 'unknown'\n",
    "    }\n",
    "carriageway_hazards_dict = {\n",
    "    0: 'none', 1: 'vehicle_load_on_road', 2: 'other_object_on_road', 3: 'previous_accident', 4: 'dog_on_road', \n",
    "    5: 'other_animal_on_road', 6: 'pedestrian_in_carriageway', 7: 'any_animal_in_carriageway_except_ridden_horse', \n",
    "    -1: 'unknown', 9: 'unknown'\n",
    "    }\n",
    "urban_or_rural_area_dict = {\n",
    "    1: 'urban', 2: 'rural', 3: 'unknown', 4: 'unknown'\n",
    "    }\n",
    "\n",
    "# Dictionary of dictionaries for renaming\n",
    "rename_dict = {\n",
    "    'accident_severity': accident_severity_dict,\n",
    "    'day_of_week': day_of_week_dict,\n",
    "    'first_road_class': first_road_class_dict,\n",
    "    'road_type': road_type_dict,\n",
    "    'junction_detail' : junction_detail_dict,\n",
    "    'junction_control' : junction_control_dict,\n",
    "    'second_road_class' : second_road_class_dict,\n",
    "    'light_conditions' : light_conditions_dict,\n",
    "    'weather_conditions' : weather_conditions_dict,\n",
    "    'road_surface_conditions' : road_surface_conditions_dict,\n",
    "    'special_conditions_at_site' : special_conditions_at_site_dict,\n",
    "    'carriageway_hazards' : carriageway_hazards_dict,\n",
    "    'urban_or_rural_area' : urban_or_rural_area_dict\n",
    "}\n",
    "\n",
    "# Loop through the dictionary to rename values\n",
    "for column, mapping_dict in rename_dict.items():\n",
    "    if column in acc_data_ML.columns: \n",
    "        acc_data_ML[column] = acc_data_ML[column].replace(mapping_dict)\n",
    "        if column != 'accident_severity':\n",
    "            acc_data_ML[column] = acc_data_ML[column].astype('category')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3691651, 15)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_data_ML.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Binning \n",
    "\n",
    "##### Speed limits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.  0.  5. 10. 15. 20. 25. 30. 40. 50. 60. 70.]\n"
     ]
    }
   ],
   "source": [
    "print(np.sort(acc_data_ML['speed_limit'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bin the speed limits into 'unknown', 'under_20', '20-29', '30-39', '40-49', '50-59', '60-69', '70_or_more'\n",
    "bin_edges = [-np.inf, -1, 19, 29, 39, 49, 59, 69, np.inf]\n",
    "bin_labels = ['unknown', 'under_20', '20-29', '30-39', '40-49', '50-59', '60-69', '70_or_more']\n",
    "acc_data_ML['speed_limit_bins'] = pd.cut(acc_data_ML['speed_limit'], bins = bin_edges, labels = bin_labels, include_lowest = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         speed_limit speed_limit_bins\n",
      "5118264         70.0       70_or_more\n",
      "5118265         70.0       70_or_more\n",
      "5118266         60.0            60-69\n",
      "5118267         70.0       70_or_more\n",
      "5118268         70.0       70_or_more\n"
     ]
    }
   ],
   "source": [
    "# Check that binning worked as intended\n",
    "print(acc_data_ML[['speed_limit', 'speed_limit_bins']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['00:00' '00:01' '00:02' ... '23:57' '23:58' '23:59']\n"
     ]
    }
   ],
   "source": [
    "print(np.sort(acc_data_ML['time'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'time' column to datetime to extract the hour\n",
    "acc_data_ML['hour'] = pd.to_datetime(acc_data_ML['time'], format='%H:%M').dt.hour\n",
    "\n",
    "# Define bin edges and labels\n",
    "# 00.00 - 05.59: 'night', 06.00 - 09.59: 'morning_rush', 10.00 - 15.59: 'day', 16.00 - 19.59: 'evening_rush',\n",
    "# 20.00 - 23.59: 'late_evening'\n",
    "bin_edges = [0, 5, 9, 15, 19, 24]\n",
    "bin_labels = ['night', 'morning_rush', 'day', 'evening_rush', 'late_evening']\n",
    "\n",
    "# Bin the time data\n",
    "acc_data_ML['time_of_day'] = pd.cut(acc_data_ML['hour'], \n",
    "                                    bins = bin_edges, labels = bin_labels, include_lowest = True, right = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          time  hour   time_of_day\n",
      "8809905  00:10     0         night\n",
      "8809906  11:41    11           day\n",
      "8809907  09:25     9           day\n",
      "8809908  01:40     1         night\n",
      "8809909  15:00    15  evening_rush\n",
      "8809910  15:00    15  evening_rush\n",
      "8809911  21:35    21  late_evening\n",
      "8809912  11:44    11           day\n",
      "8809913  16:45    16  evening_rush\n",
      "8809914  19:05    19  late_evening\n"
     ]
    }
   ],
   "source": [
    "# Check that binning worked as intended\n",
    "print(acc_data_ML[['time', 'hour', 'time_of_day']].tail(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove original speed limit, time and hour columns from the dataset\n",
    "cols_remove = ['speed_limit', 'time', 'hour']\n",
    "acc_data_ML = acc_data_ML.drop(cols_remove, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3691651, 15)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_data_ML.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Remove unknowns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows where any column has 'unknown' as value\n",
    "acc_data_ML_filtered = acc_data_ML.loc[~(acc_data_ML.isin(['unknown']).any(axis=1))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of rows removed: 82.65%\n",
      "(640359, 15)\n"
     ]
    }
   ],
   "source": [
    "# Check how many rows were affected\n",
    "percentage_removed = (len(acc_data_ML) - len(acc_data_ML_filtered)) / len(acc_data_ML) * 100\n",
    "\n",
    "print(f\"Percentage of rows removed: {percentage_removed:.2f}%\")\n",
    "print(acc_data_ML_filtered.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accident_severity                   0\n",
      "day_of_week                         0\n",
      "first_road_class              1110250\n",
      "road_type                       32598\n",
      "junction_detail                  8852\n",
      "junction_control              1431745\n",
      "second_road_class             1516396\n",
      "light_conditions                   33\n",
      "weather_conditions              73388\n",
      "road_surface_conditions         13882\n",
      "special_conditions_at_site      13387\n",
      "carriageway_hazards             11946\n",
      "urban_or_rural_area              3901\n",
      "speed_limit_bins                  129\n",
      "time_of_day                         0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Which features contain most unknown values\n",
    "\n",
    "unknown_counts = acc_data_ML.apply(lambda x: (x == 'unknown').sum())\n",
    "print(unknown_counts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quite a major chunk of data removed, but still have quite many records. Most unknowns from first_road_class, junction_control and second_road_class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Split the data into train, validation, and test sets\n",
    "\n",
    "Use ratio of 70:15:15 for train, validation and test set, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train (70%) and temp (30%) datasets\n",
    "train_data, temp_data = train_test_split(acc_data_ML_filtered, test_size = 0.3, random_state = 33)\n",
    "\n",
    "# Split the temp dataset into validation and test datasets (50% each of temp_data)\n",
    "val_data, test_data = train_test_split(temp_data, test_size = 0.5, random_state = 33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting features and labels\n",
    "X_train = train_data.drop('accident_severity', axis=1)\n",
    "y_train = train_data['accident_severity']\n",
    "\n",
    "X_val = val_data.drop('accident_severity', axis=1)\n",
    "y_val = val_data['accident_severity']\n",
    "\n",
    "X_test = test_data.drop('accident_severity', axis=1)\n",
    "y_test = test_data['accident_severity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(448251, 14)\n",
      "(448251,)\n",
      "(96054, 14)\n",
      "(96054,)\n",
      "(96054, 14)\n",
      "(96054,)\n"
     ]
    }
   ],
   "source": [
    "# Check the sizes of datasets\n",
    "print(X_train.shape) #(448251, 14)\n",
    "print(y_train.shape) #(448251,)\n",
    "\n",
    "print(X_val.shape) #(96054, 14)\n",
    "print(y_val.shape) #(96054,)\n",
    "\n",
    "print(X_test.shape) #(96054, 14)\n",
    "print(y_test.shape) #(96054,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. One-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 448251 entries, 6998066 to 5735441\n",
      "Data columns (total 14 columns):\n",
      " #   Column                      Non-Null Count   Dtype   \n",
      "---  ------                      --------------   -----   \n",
      " 0   day_of_week                 448251 non-null  category\n",
      " 1   first_road_class            448251 non-null  category\n",
      " 2   road_type                   448251 non-null  category\n",
      " 3   junction_detail             448251 non-null  category\n",
      " 4   junction_control            448251 non-null  category\n",
      " 5   second_road_class           448251 non-null  category\n",
      " 6   light_conditions            448251 non-null  category\n",
      " 7   weather_conditions          448251 non-null  category\n",
      " 8   road_surface_conditions     448251 non-null  category\n",
      " 9   special_conditions_at_site  448251 non-null  category\n",
      " 10  carriageway_hazards         448251 non-null  category\n",
      " 11  urban_or_rural_area         448251 non-null  category\n",
      " 12  speed_limit_bins            448251 non-null  category\n",
      " 13  time_of_day                 448251 non-null  category\n",
      "dtypes: category(14)\n",
      "memory usage: 9.4 MB\n"
     ]
    }
   ],
   "source": [
    "# Confirm all features are categorical\n",
    "X_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../00_data/encoder.joblib']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = OneHotEncoder(sparse_output = False, handle_unknown = 'ignore')\n",
    "\n",
    "# Fit and transform the train data\n",
    "X_train_encoded = pd.DataFrame(encoder.fit_transform(X_train), columns = encoder.get_feature_names_out())\n",
    "\n",
    "# Transform the validation and test data\n",
    "X_val_encoded = pd.DataFrame(encoder.transform(X_val), columns = encoder.get_feature_names_out())\n",
    "X_test_encoded = pd.DataFrame(encoder.transform(X_test), columns = encoder.get_feature_names_out())\n",
    "\n",
    "# Saving the encoder\n",
    "dump(encoder, '../00_data/encoder.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(448251, 79)\n",
      "(96054, 79)\n",
      "(96054, 79)\n"
     ]
    }
   ],
   "source": [
    "# Check dimensions of encoded datasets (confirm that number or records are the same and number of features increased)\n",
    "print(X_train_encoded.shape) #(448251, 79)\n",
    "\n",
    "print(X_val_encoded.shape) #(96054, 79)\n",
    "\n",
    "print(X_test_encoded.shape) #(96054, 79)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. Balance the train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Oversampling using SMOTE on the encoded train data\n",
    "smote = SMOTE(random_state = 33)\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train_encoded, y_train)\n",
    "\n",
    "# Undersampling using RandomUnderSampler on the encoded train data\n",
    "rus = RandomUnderSampler(random_state = 33)\n",
    "X_train_rus, y_train_rus = rus.fit_resample(X_train_encoded, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combined under- and oversampling (ensemble sampling)\n",
    "\n",
    "# Define undersampling strategy\n",
    "under = RandomUnderSampler(sampling_strategy = 0.5)  # This will reduce the majority class to be double the minority\n",
    "\n",
    "# Define oversampling strategy\n",
    "over = SMOTE(sampling_strategy = 1.0)  # This will balance the two classes\n",
    "\n",
    "# Combine the sampling strategies into a pipeline\n",
    "pipeline = Pipeline(steps = [('under', under), ('over', over)])\n",
    "\n",
    "# Apply the pipeline to your data\n",
    "X_train_ensemble, y_train_ensemble = pipeline.fit_resample(X_train_encoded, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(448251, 79)\n",
      "(448251,)\n",
      "(888216, 79)\n",
      "(888216,)\n",
      "(8286, 79)\n",
      "(8286,)\n",
      "(16572, 79)\n",
      "(16572,)\n"
     ]
    }
   ],
   "source": [
    "# Check the dimensions of the original, oversampled and undersampled train data\n",
    "print(X_train_encoded.shape) # (448251, 79)\n",
    "print(y_train.shape) # (448251,)\n",
    "\n",
    "print(X_train_smote.shape) # (888216, 79)\n",
    "print(y_train_smote.shape) # (888216,)\n",
    "\n",
    "print(X_train_rus.shape) # (8286, 79)\n",
    "print(y_train_rus.shape) # (8286,)\n",
    "\n",
    "print(X_train_ensemble.shape) # (33144, 79)\n",
    "print(y_train_ensemble.shape) # (33144,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10. Save the train, validation, and test sets as .csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Training data\n",
    "# Original\n",
    "X_train_encoded.to_csv('../00_data/X_train_orig_road_acc.csv', index = False)\n",
    "y_train.to_csv('../00_data/y_train_orig_road_acc.csv', index = False)\n",
    "\n",
    "# Oversampled\n",
    "X_train_smote.to_csv('../00_data/X_train_oversamp_road_acc.csv', index = False)\n",
    "y_train_smote.to_csv('../00_data/y_train_oversamp_road_acc.csv', index = False)\n",
    "\n",
    "# Undersampled\n",
    "X_train_rus.to_csv('../00_data/X_train_undersamp_road_acc.csv', index = False)\n",
    "y_train_rus.to_csv('../00_data/y_train_undersamp_road_acc.csv', index = False)\n",
    "\n",
    "# Ensemble resampled\n",
    "X_train_ensemble.to_csv('../00_data/X_train_ensemble_road_acc.csv', index = False)\n",
    "y_train_ensemble.to_csv('../00_data/y_train_ensemble_road_acc.csv', index = False)\n",
    "\n",
    "# Validation data\n",
    "X_val_encoded.to_csv('../00_data/X_val_road_acc.csv', index = False)\n",
    "y_val.to_csv('../00_data/y_val_road_acc.csv', index = False)\n",
    "\n",
    "# Test data\n",
    "X_test_encoded.to_csv('../00_data/X_test_road_acc.csv', index = False)\n",
    "y_test.to_csv('../00_data/y_test_road_acc.csv', index = False)\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
